import argparse
import os
import cv2
import numpy as np
import torch
import matplotlib.pyplot as plt
import glob
import albumentations as A
from albumentations.pytorch import ToTensorV2

# Import c√°c module t·ª´ source code c·ªßa b·∫°n
# ƒê·∫£m b·∫£o b·∫°n ƒë·∫∑t file inference.py c√πng c·∫•p v·ªõi folder src/
from src.model import MedicalConceptModel
from src.dataset import TARGET_CLASSES


def get_args():
    parser = argparse.ArgumentParser(
        description="Inference VinDr-CXR with Saliency Maps"
    )

    # Tham s·ªë b·∫Øt bu·ªôc
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file ·∫£nh (.jpg, .png, .dicom) HO·∫∂C ƒë∆∞·ªùng d·∫´n folder ch·ª©a ·∫£nh",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file weights (.pth) ƒë√£ train",
    )

    # Tham s·ªë tu·ª≥ ch·ªçn
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Ng∆∞·ª°ng x√°c su·∫•t ƒë·ªÉ hi·ªÉn th·ªã b·ªánh (Default: 0.5)",
    )
    parser.add_argument(
        "--img_size",
        type=int,
        default=384,
        help="K√≠ch th∆∞·ªõc ·∫£nh input cho model (Default: 384)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="Thi·∫øt b·ªã ch·∫°y inference: 'cuda' ho·∫∑c 'cpu'",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="predictions",
        help="Folder l∆∞u ·∫£nh k·∫øt qu·∫£ (n·∫øu kh√¥ng set, s·∫Ω ch·ªâ in ra m√†n h√¨nh)",
    )
    parser.add_argument(
        "--no_show",
        action="store_true",
        help="N·∫øu d√πng flag n√†y, code s·∫Ω KH√îNG popup c·ª≠a s·ªï ·∫£nh (d√πng khi ch·∫°y tr√™n server)",
    )

    return parser.parse_args()


def get_val_transform(img_size):
    return A.Compose(
        [
            A.Resize(img_size, img_size),
            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
            ToTensorV2(),
        ]
    )


def predict_and_visualize(model, image_path, args, transform):
    # 1. ƒê·ªçc ·∫£nh
    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ·∫£nh DICOM n·∫øu c·∫ßn (·ªü ƒë√¢y demo v·ªõi ·∫£nh th∆∞·ªùng jpg/png)
    if not os.path.exists(image_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {image_path}")
        return

    original_img = cv2.imread(image_path)
    if original_img is None:
        print(f"‚ùå L·ªói ƒë·ªçc ·∫£nh: {image_path}")
        return

    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
    h_orig, w_orig, _ = original_img.shape

    # 2. Preprocess
    augmented = transform(image=original_img)
    input_tensor = augmented["image"].unsqueeze(0).to(args.device)

    # 3. Inference
    with torch.no_grad():
        outputs = model(input_tensor)

    logits = outputs["logits"][0]
    attn_maps = outputs["attn_maps"][0]  # [Num_Classes, H, W]

    probs = torch.sigmoid(logits).cpu().numpy()

    # 4. X·ª≠ l√Ω k·∫øt qu·∫£
    active_indices = np.where(probs > args.threshold)[0]
    filename = os.path.basename(image_path)

    print(f"\n--- üì∏ K·∫øt qu·∫£ cho: {filename} ---")

    if len(active_indices) == 0:
        print("‚úÖ K·∫øt lu·∫≠n: B√¨nh th∆∞·ªùng / Kh√¥ng ph√°t hi·ªán b·ªánh (No findings)")
        # V·∫´n l∆∞u ·∫£nh g·ªëc n·∫øu c·∫ßn
        if args.output_dir:
            save_path = os.path.join(args.output_dir, f"clean_{filename}")
            plt.imsave(save_path, original_img)
        return

    # N·∫øu c√≥ b·ªánh -> V·∫Ω Heatmap ch·ªìng l√™n
    for idx in active_indices:
        disease_name = TARGET_CLASSES[idx]
        score = probs[idx]
        print(f"‚ö†Ô∏è Ph√°t hi·ªán: {disease_name} (Score: {score:.2f})")

        # X·ª≠ l√Ω Heatmap
        heatmap = attn_maps[idx].cpu().numpy()

        # Normalize v·ªÅ 0-255
        heatmap = heatmap - np.min(heatmap)
        heatmap = heatmap / (np.max(heatmap) + 1e-8)
        heatmap = np.uint8(255 * heatmap)

        # Resize v·ªÅ k√≠ch th∆∞·ªõc g·ªëc
        heatmap_resized = cv2.resize(heatmap, (w_orig, h_orig))

        # T·∫°o m√†u
        heatmap_color = cv2.applyColorMap(heatmap_resized, cv2.COLORMAP_JET)

        # Ch·ªìng ·∫£nh (Overlay)
        # Chuy·ªÉn heatmap_color t·ª´ BGR sang RGB ƒë·ªÉ hi·ªÉn th·ªã ƒë√∫ng b·∫±ng matplotlib
        heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)
        overlay = cv2.addWeighted(original_img, 0.6, heatmap_color, 0.4, 0)

        # 5. L∆∞u ho·∫∑c Hi·ªÉn th·ªã
        title = f"{disease_name} ({score:.2f})"

        # L∆∞u ra file
        if args.output_dir:
            os.makedirs(args.output_dir, exist_ok=True)
            # T√™n file: tenanh_tenbenh.jpg
            save_name = (
                f"{os.path.splitext(filename)[0]}_{disease_name.replace(' ', '_')}.jpg"
            )
            save_path = os.path.join(args.output_dir, save_name)

            plt.figure(figsize=(10, 10))
            plt.imshow(overlay)
            plt.title(title, fontsize=15, color="red")
            plt.axis("off")
            plt.savefig(save_path, bbox_inches="tight")
            plt.close()  # ƒê√≥ng figure ƒë·ªÉ gi·∫£i ph√≥ng ram
            print(f"   üíæ ƒê√£ l∆∞u ·∫£nh ph√¢n t√≠ch t·∫°i: {save_path}")

        # Hi·ªÉn th·ªã popup (n·∫øu kh√¥ng ch·∫∑n)
        if not args.no_show:
            plt.figure(figsize=(6, 6))
            plt.imshow(overlay)
            plt.title(title)
            plt.axis("off")
            plt.show()


def main():
    args = get_args()

    # 1. Setup Model
    print(f"‚è≥ ƒêang load model t·ª´ {args.checkpoint}...")
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    # Kh·ªüi t·∫°o ki·∫øn tr√∫c model (ph·∫£i kh·ªõp v·ªõi l√∫c train)
    model = MedicalConceptModel(num_classes=len(TARGET_CLASSES))

    # Load weights
    try:
        state_dict = torch.load(args.checkpoint, map_location=device)
        model.load_state_dict(state_dict)
    except Exception as e:
        print(f"‚ùå L·ªói load checkpoint: {e}")
        print("G·ª£i √Ω: Ki·ªÉm tra l·∫°i ki·∫øn tr√∫c model ho·∫∑c ƒë∆∞·ªùng d·∫´n file.")
        return

    model.to(device)
    model.eval()
    print("‚úÖ Load model th√†nh c√¥ng!")

    # 2. Setup Transform
    val_transform = get_val_transform(args.img_size)

    # 3. X√°c ƒë·ªãnh danh s√°ch ·∫£nh c·∫ßn ch·∫°y
    image_paths = []
    if os.path.isdir(args.input):
        # N·∫øu input l√† folder, l·∫•y t·∫•t c·∫£ ·∫£nh jpg/png/jpeg
        types = ("*.jpg", "*.jpeg", "*.png", "*.bmp")
        for t in types:
            image_paths.extend(glob.glob(os.path.join(args.input, t)))
        print(f"üìÇ T√¨m th·∫•y {len(image_paths)} ·∫£nh trong th∆∞ m·ª•c.")
    else:
        # N·∫øu input l√† file ƒë∆°n l·∫ª
        image_paths = [args.input]

    # 4. Ch·∫°y v√≤ng l·∫∑p d·ª± ƒëo√°n
    for img_path in image_paths:
        predict_and_visualize(model, img_path, args, val_transform)


if __name__ == "__main__":
    main()
