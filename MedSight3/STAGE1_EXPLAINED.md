# Stage 1: Concept Learning - Detailed Explanation

## ðŸŽ¯ Goal of Stage 1

**Train the model to:**
1. Identify which anatomical concepts are present in an X-ray
2. Localize WHERE each concept appears (via CAMs)

**Output:** A model that can produce interpretable **Concept Activation Maps (CAMs)** showing spatial locations of medical findings.

---

## ðŸ“Š Data Flow Diagram

```
INPUT IMAGE                                    LABELS
(224Ã—224 RGB)                                  concepts: [0,1,0,1,...]  (K=22 concepts)
     â†“                                         targets:  [0,0,1,0,...]  (C=6 diseases)
     â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                                                  â”‚
     â†“                                                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚  MedMAE Backboneâ”‚  (Pre-trained ViT)                                â”‚
â”‚   (FROZEN or    â”‚                                                    â”‚
â”‚   Fine-tuning)  â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
     â†“                                                                  â”‚
  Features f                                                            â”‚
  (B, 768, 7, 7)  â† 768 channels, 7Ã—7 spatial grid                    â”‚
     â†“                                                                  â”‚
     â”‚                                                                  â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
     â”‚                    â”‚                 â”‚                      â”‚  â”‚
     â†“                    â†“                 â†“                      â†“  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ Concept  â”‚      â”‚ Projector  â”‚   â”‚ Prototypes  â”‚      â”‚   Task   â”‚â”‚
â”‚   Head   â”‚      â”‚   (not     â”‚   â”‚   (not      â”‚      â”‚   Head   â”‚â”‚
â”‚          â”‚      â”‚   trained) â”‚   â”‚   trained)  â”‚      â”‚   (not   â”‚â”‚
â”‚ (1Ã—1 Conv)â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ trained) â”‚â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
     â†“                                                                 â”‚
   CAMs                                                                â”‚
 (B, 22, 7, 7)  â† One heatmap per concept                             â”‚
     â†“                                                                 â”‚
     â”‚                                                                 â”‚
     â†“                                                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
â”‚  Global Max Pooling  â”‚  â† Extract strongest activation per concept â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚
     â†“                                                                 â”‚
Concept Logits                                                         â”‚
  (B, 22)                                                              â”‚
     â†“                                                                 â”‚
     â”‚                                                                 â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BCEWithLogitsLoss      â”‚
â”‚ (with pos_weight)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
   Loss â†’ Backprop â†’ Update weights
```

---

## ðŸ”„ Step-by-Step Data Flow

### **Step 1: Load Training Batch**

```python
batch = {
    'image': torch.tensor([B, 3, 224, 224]),      # X-ray images
    'concepts': torch.tensor([B, 22]),            # Concept labels (binary)
    'targets': torch.tensor([B, 6]),              # Disease labels (unused in Stage 1)
    'bboxes': [list of bbox dicts]                # Optional: bbox annotations
}
```

**Example:**
- Image: Chest X-ray of a patient
- Concepts: `[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...]` 
  - Index 1 (Atelectasis): Present âœ“
  - Index 4 (Cardiomegaly): Present âœ“
  - Index 10 (Lung Opacity): Present âœ“
  - Others: Absent âœ—

### **Step 2: Feature Extraction (Backbone)**

```python
f = model.backbone(images)  # (B, 768, 14, 14)
```

**What happens:**
- MedMAE (ViT-Base) processes 224Ã—224 image
- Outputs feature maps: **768 channels Ã— 14Ã—14 spatial grid**
- Each 14Ã—14 location has receptive field of ~16Ã—16 pixels in original image (patch size)
- Features encode high-level semantic information

**Learning rate:** Very slow (`lr * 0.01`) to preserve pre-trained medical knowledge

### **Step 3: Generate Concept Activation Maps**

```python
cams = model.concept_head(f)  # (B, 22, 14, 14)
```

**What happens:**
- **1Ã—1 Convolution**: `Conv2d(768, 22, kernel_size=1)`
- Transforms 768-dim features â†’ 22 concept channels
- **No activation function** (raw logits for BCEWithLogitsLoss)

**Output interpretation:**
- `cams[batch_idx, concept_idx, y, x]` = activation strength at spatial location (x, y)
- High value â†’ concept likely present at this location
- Low/negative value â†’ concept likely absent

**Example CAM for "Cardiomegaly":**
```
CAM[0, 4] = 14Ã—14 heatmap showing heart region
  Center rows (6-9) have highest activations (1.5-1.8)
  Edges have low/negative values (-0.4 to 0.2)
```
â†’ Peak at center (3, 3) where heart is located! â¤ï¸

### **Step 4: Pool to Concept Predictions**

```python
concept_logits = F.adaptive_max_pool2d(cams, (1, 1))  # (B, 22)
concept_logits = concept_logits.squeeze(-1).squeeze(-1)
```

**What happens:**
- **Global Max Pooling** across spatial dimensions (7Ã—7 â†’ 1Ã—1)
- Takes the **strongest activation** from each concept's CAM
- Result: One score per concept indicating its presence

**Why max pooling (not average)?**
- Concepts may occupy small regions (nodules, fractures)
- Max captures "exists somewhere in image"
- Average would dilute signal from small findings

**Example:**
```python
cams[0, 4].max() = 1.8  â†’ High confidence Cardiomegaly present
cams[0, 7].max() = -0.9 â†’ Emphysema likely absent
```

### **Step 5: Compute Loss**

#### **5a. Standard Training (Image-level labels only)**

```python
loss = BCEWithLogitsLoss(pos_weight)(concept_logits, concepts_gt)
```

**What happens:**
- Binary cross-entropy between predictions and ground truth
- `pos_weight` handles class imbalance (rare concepts get higher weight)
- Loss encourages:
  - High logits for present concepts (label=1)
  - Low logits for absent concepts (label=0)

**Example:**
```python
concept_logits[0] = [âˆ’0.5, 1.8, âˆ’1.2, 0.3, 1.5, ...]  # Model predictions
concepts_gt[0]    = [   0,   1,    0,   0,   1, ...]  # Ground truth

# Losses per concept:
BCE([âˆ’0.5], [0]) = 0.47  # Correct (low for absent)
BCE([1.8],  [1]) = 0.15  # Correct (high for present)
BCE([âˆ’1.2], [0]) = 0.26  # Correct
BCE([0.3],  [0]) = 0.85  # WRONG! Should be negative
BCE([1.5],  [1]) = 0.20  # Correct

# Weighted average â†’ Total loss
```

#### **5b. BBox-Supervised Training (Spatial labels)**

```python
loss = BBoxGuidedConceptLoss(cams, concepts_gt, bboxes)
     = Î± * classification_loss + Î² * localization_loss
```

**Classification Loss (same as above):**
```python
classification_loss = BCE(max_pool(cams), concepts_gt)
```

**Localization Loss (NEW):**
```python
For each bbox [concept_idx=4, x_min=0.3, y_min=0.4, x_max=0.7, y_max=0.8]:
    # Create spatial mask
    inside_mask  = [0 0 0 0 0 0 0]
                   [0 0 1 1 1 0 0]  â† 1 inside bbox
                   [0 0 1 1 1 0 0]
                   [0 0 1 1 1 0 0]
                   [0 0 0 0 0 0 0]
    
    outside_mask = 1 - inside_mask
    
    # Loss: CAM should be HIGH inside, LOW outside
    L_inside  = mean(relu(-cams[4] * inside_mask))   # Penalize negative values inside
    L_outside = mean(relu(cams[4] * outside_mask))   # Penalize positive values outside
    
    localization_loss = L_inside + L_outside
```

**Effect:**
- Model learns **exact spatial locations** of concepts
- CAMs become sharper and more interpretable
- Better initialization for Stage 2

### **Step 6: Backpropagation**

```python
scaler.scale(loss).backward()
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
scaler.step(optimizer)
```

**What gets updated:**
- âœ… **Concept Head weights** (main learning)
- âœ… **Backbone weights** (fine-tuning, slow LR)
- âŒ Projector (frozen, not used in Stage 1)
- âŒ Prototypes (frozen, not used in Stage 1)
- âŒ Task Head (frozen, not used in Stage 1)

**Gradient flow:**
```
Loss â†’ concept_logits â†’ max_pool â†’ CAMs â†’ concept_head â†’ features â†’ backbone
```

---

## ðŸ“ˆ What the Model Learns

### **Epoch 1-3: Initial Adaptation**
- Backbone: Adapts pre-trained features to chest X-rays
- Concept head: Learns rough associations (e.g., bright region â†’ opacity)
- CAMs: Very noisy, scattered activations

### **Epoch 4-10: Concept Discovery**
- Model identifies discriminative regions
- CAMs start focusing on relevant anatomy
- Common concepts (opacity, cardiomegaly) learned first
- Rare concepts (fractures) still struggling

### **Epoch 11-20: Refinement**
- CAMs become sharper and more localized
- Spatial patterns stabilize
- Model distinguishes similar concepts (infiltration vs consolidation)

### **With BBox Supervision:**
- **Faster convergence** (10-15 epochs vs 20)
- **Precise localization** from early epochs
- **Higher interpretability** throughout training

---

## ðŸŽ¨ Visual Example: CAM Evolution

**Ground Truth:**
- Image: Chest X-ray with left lower lobe pneumonia
- Labels: Consolidation=1, Lung Opacity=1

**Epoch 1:**
```
CAM[Consolidation]:     CAM[Lung Opacity]:
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘]               [â–“â–“â–‘â–‘â–‘â–‘â–‘]
[â–‘â–‘â–‘â–“â–‘â–‘â–‘]               [â–“â–“â–“â–‘â–‘â–‘â–‘]
[â–‘â–‘â–“â–“â–‘â–‘â–‘]  â† Noisy      [â–“â–“â–“â–“â–‘â–‘â–‘]  â† Diffuse
[â–‘â–‘â–‘â–“â–‘â–‘â–‘]               [â–“â–“â–“â–‘â–‘â–‘â–‘]
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘]               [â–“â–“â–‘â–‘â–‘â–‘â–‘]
```

**Epoch 10:**
```
CAM[Consolidation]:     CAM[Lung Opacity]:
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘]               [â–‘â–‘â–‘â–‘â–‘â–‘â–‘]
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘]               [â–‘â–‘â–“â–“â–“â–‘â–‘]
[â–‘â–‘â–“â–“â–“â–‘â–‘]  â† Focused    [â–‘â–“â–“â–“â–“â–“â–‘]  â† Covers area
[â–‘â–‘â–“â–“â–“â–‘â–‘]               [â–‘â–‘â–“â–“â–“â–‘â–‘]
[â–‘â–‘â–‘â–“â–‘â–‘â–‘]               [â–‘â–‘â–‘â–‘â–‘â–‘â–‘]
```

**Epoch 20 (Final):**
```
CAM[Consolidation]:     CAM[Lung Opacity]:
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘]               [â–‘â–‘â–‘â–‘â–‘â–‘â–‘]
[â–‘â–‘â–‘â–‘â–‘â–‘â–‘]               [â–‘â–‘â–‘â–“â–‘â–‘â–‘]
[â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘]  â† Sharp!     [â–‘â–‘â–“â–“â–“â–“â–‘]  â† Broader
[â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–‘]               [â–‘â–“â–“â–“â–“â–“â–‘]
[â–‘â–‘â–‘â–‘â–ˆâ–‘â–‘]               [â–‘â–‘â–“â–“â–“â–‘â–‘]
```

---

## ðŸ“Š Metrics During Training

### **Training Loss**
- Starts: ~0.6-0.8 (random initialization)
- Target: ~0.3-0.5 (converged)
- Oscillates if LR too high â†’ reduce to 1e-4

### **Validation Loss**
- Should track training loss closely
- Gap > 0.1 â†’ overfitting (use early stopping)

### **Concept AUC**
- Random: 0.50
- Epoch 5: ~0.65-0.70
- Epoch 10: ~0.72-0.78
- **Target: â‰¥ 0.75** (good), **â‰¥ 0.80** (excellent)

### **Per-Concept Performance**
```
Easy concepts (AUC > 0.85):
  - Cardiomegaly (large, obvious)
  - Pleural Effusion (distinct pattern)
  
Medium concepts (AUC 0.70-0.85):
  - Lung Opacity (common but varied)
  - Consolidation (overlaps with others)
  
Hard concepts (AUC < 0.70):
  - Rib Fracture (subtle, small)
  - Lung Cyst (rare in training data)
```

---

## ðŸ’¾ Stage 1 Output

### **Saved Checkpoint: `best_model_stage1.pth`**

Contains:
```python
{
    'backbone': weights,        # Fine-tuned MedMAE
    'concept_head': weights,    # Trained 1Ã—1 Conv
    'projector': weights,       # Untrained (random init)
    'prototypes': weights,      # Untrained (random init)
    'task_head': weights        # Untrained (random init)
}
```

### **What You Can Do:**

1. **Visualize CAMs:**
```bash
python visualize_cams.py --checkpoint best_model_stage1.pth --image test.png
```

2. **Predict Concepts:**
```python
model.load_state_dict(torch.load('best_model_stage1.pth'))
outputs = model(image)
cams = outputs['cams']  # (1, 22, 7, 7) heatmaps
concept_scores = cams.amax(dim=(-1, -2))  # (1, 22) predictions
```

3. **Proceed to Stage 2:**
```python
# Load Stage 1 weights, freeze backbone & concept_head
# Train projector & prototypes with contrastive loss
```

---

## ðŸŽ¯ Key Takeaways

1. **Stage 1 is about concept localization**, not disease prediction
2. **CAMs are the core output** - they become prototypes in Stage 2
3. **Max pooling connects CAMs to labels** - strongest activation determines presence
4. **BBox supervision significantly improves** CAM quality (optional but recommended)
5. **Slow learning rates preserve** pre-trained knowledge
6. **Target AUC â‰¥ 0.75** for good Stage 1 performance

**Next:** Stage 2 uses these CAMs to learn prototypical representations of each concept! ðŸš€
