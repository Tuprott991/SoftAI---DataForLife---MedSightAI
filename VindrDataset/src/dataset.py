import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
import os
import cv2  # Thay pydicom b·∫±ng opencv
from PIL import Image


class VINDRCXRDataset(Dataset):
    def __init__(self, csv_file, image_dir, num_classes, target_size, map_size, exclude_classes=None):
        """
        Kh·ªüi t·∫°o Dataset cho file PNG.

        Args:
            exclude_classes: List c√°c class c·∫ßn lo·∫°i b·ªè (v√≠ d·ª•: ['Consolidation', 'ILD', ...])
        """
        # ƒê·ªçc CSV
        self.data_df = pd.read_csv(csv_file)
        self.image_dir = image_dir
        self.target_size = target_size
        self.map_size = map_size
        self.exclude_classes = exclude_classes or []

        # 1. Chu·∫©n b·ªã mapping Class ID
        self.image_ids = self.data_df["image_id"].unique().tolist()
        class_mapping_data = self.data_df[["class_name", "class_id"]].drop_duplicates()

        self.class_to_id = {}
        self.id_to_class = {}
        new_class_id = 0

        for _, row in class_mapping_data.iterrows():
            name = row["class_name"]
            original_id = row["class_id"]

            # B·ªè qua "No finding" v√† c√°c class trong exclude list
            if name.lower() == "no finding":
                continue
            if name in self.exclude_classes:
                continue
            if 0 <= original_id <= 13:
                self.class_to_id[name] = new_class_id
                self.id_to_class[new_class_id] = name
                new_class_id += 1

        # C·∫≠p nh·∫≠t num_classes d·ª±a tr√™n s·ªë class th·ª±c t·∫ø sau khi l·ªçc
        self.num_classes = len(self.class_to_id)
        print(f"üìä Dataset initialized with {self.num_classes} classes (excluded: {len(self.exclude_classes)})")
        print(f"   Classes: {list(self.class_to_id.keys())}")

        # 2. T·ªëi ∆∞u h√≥a vi·ªác tra c·ª©u k√≠ch th∆∞·ªõc g·ªëc (N·∫øu c√≥ trong CSV)
        # T·∫°o dictionary ƒë·ªÉ tra c·ª©u nhanh width/height n·∫øu CSV c√≥ c·ªôt n√†y
        self.has_dim_info = (
            "width" in self.data_df.columns and "height" in self.data_df.columns
        )

        if self.has_dim_info:
            # B∆Ø·ªöC QUAN TR·ªåNG:
            # 1. Ch·ªâ l·∫•y 3 c·ªôt c·∫ßn thi·∫øt
            # 2. Lo·∫°i b·ªè c√°c d√≤ng tr√πng image_id (ƒë·ªÉ m·ªói ·∫£nh ch·ªâ c√≤n 1 d√≤ng duy nh·∫•t ch·ª©a width/height)
            unique_dims = self.data_df[["image_id", "width", "height"]].drop_duplicates(
                subset=["image_id"]
            )

            # 3. Gi·ªù th√¨ set_index s·∫Ω an to√†n v√† to_dict s·∫Ω nhanh h∆°n nhi·ªÅu
            self.dim_lookup = unique_dims.set_index("image_id").to_dict("index")
        else:
            self.dim_lookup = {}

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        image_id = self.image_ids[idx]

        # L·∫•y c√°c d√≤ng annotation c·ªßa ·∫£nh n√†y
        img_annotations = self.data_df[self.data_df["image_id"] == image_id]

        # --- 1. Load ·∫¢nh PNG (Thay th·∫ø ph·∫ßn DICOM) ---
        img_path = os.path.join(self.image_dir, f"{image_id}.png")

        # ƒê·ªçc ·∫£nh grayscale (ƒë·ªÉ kh·ªõp v·ªõi logic c≈© l√† 1 channel)
        # N·∫øu mu·ªën d√πng 3 k√™nh m√†u RGB th√¨ b·ªè flag cv2.IMREAD_GRAYSCALE
        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)

        if image is None:  # X·ª≠ l√Ω l·ªói n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ·∫£nh
            return None, None, None, image_id

        # --- 2. X√°c ƒë·ªãnh K√≠ch th∆∞·ªõc G·ªëc (QUAN TR·ªåNG ƒê·ªÇ T√çNH BBOX) ---
        # N·∫øu CSV c√≥ th√¥ng tin width/height g·ªëc, ta ∆∞u ti√™n d√πng n√≥
        # (V√¨ n·∫øu ·∫£nh PNG ƒë√£ b·ªã resize tr∆∞·ªõc ƒë√≥, image.shape s·∫Ω sai l·ªách v·ªõi bbox g·ªëc)
        if self.has_dim_info:
            dims = self.dim_lookup[image_id]
            original_w = dims["width"]
            original_h = dims["height"]
        else:
            # N·∫øu CSV kh√¥ng c√≥, ƒë√†nh ph·∫£i d√πng k√≠ch th∆∞·ªõc th·∫≠t c·ªßa ·∫£nh v·ª´a ƒë·ªçc
            # L∆∞u √Ω: N·∫øu ·∫£nh n√†y ƒë√£ b·ªã resize (vd 512x512) th√¨ bbox s·∫Ω b·ªã t√≠nh sai!
            original_h, original_w = image.shape[:2]

        if original_w <= 0 or original_h <= 0:
            return None, None, None, image_id

        # --- 3. Resize & Normalize ---
        # Resize v·ªÅ target_size (v√≠ d·ª• 384x384)
        image = cv2.resize(image, (self.target_size, self.target_size))

        # Normalize 0-255 -> 0.0-1.0
        image = image.astype(np.float32) / 255.0

        # Chuy·ªÉn sang Tensor [1, H, W]
        image = torch.from_numpy(image).unsqueeze(0)

        # --- 4. T·∫°o Label & Attention Map (Logic gi·ªØ nguy√™n) ---
        cls_label = torch.zeros(self.num_classes, dtype=torch.float32)
        present_classes = img_annotations["class_name"].unique()

        for cls_name in present_classes:
            cls_id = self.class_to_id.get(cls_name)
            if cls_id is not None:
                cls_label[cls_id] = 1.0

        # T·∫°o Map
        attn_maps = torch.zeros(
            (self.num_classes, self.map_size, self.map_size), dtype=torch.float32
        )

        for _, row in img_annotations.iterrows():
            cls_name = row["class_name"]
            cls_id = self.class_to_id.get(cls_name)

            if cls_id is None:
                continue
            if pd.isna(row[["x_min", "y_min", "x_max", "y_max"]]).any():
                continue

            # T√≠nh t·ªça ƒë·ªô tr√™n Map 12x12 d·ª±a tr√™n t·ª∑ l·ªá v·ªõi k√≠ch th∆∞·ªõc g·ªëc
            x_min = int(row["x_min"] * (self.map_size / original_w))
            y_min = int(row["y_min"] * (self.map_size / original_h))
            x_max = int(row["x_max"] * (self.map_size / original_w))
            y_max = int(row["y_max"] * (self.map_size / original_h))

            x_min = max(0, x_min)
            y_min = max(0, y_min)
            x_max = min(self.map_size, x_max)
            y_max = min(self.map_size, y_max)

            if x_max > x_min and y_max > y_min:
                attn_maps[cls_id, y_min:y_max, x_min:x_max] = 1.0

        return image, cls_label, attn_maps, image_id
