import torch
import torch.nn as nn
from transformers import ViTModel, ViTConfig

class MedMAEBackbone(nn.Module):
    def __init__(self, model_name='facebook/vit-mae-base', pretrained_weights=None):
        super(MedMAEBackbone, self).__init__()
        # Load pre-trained ViT/MAE
        # Náº¿u model_name lÃ  HuggingFace repo â†’ load tá»« HF
        # Náº¿u pretrained_weights Ä‘Æ°á»£c cung cáº¥p â†’ load local weights
        
        if pretrained_weights and pretrained_weights.endswith('.pth'):
            print(f"ðŸ“¦ Loading MedMAE weights from {pretrained_weights}")
            # PyTorch 2.6+: weights_only=False Ä‘á»ƒ load checkpoint vá»›i argparse.Namespace
            checkpoint = torch.load(pretrained_weights, map_location='cpu', weights_only=False)
            
            # Extract model weights náº¿u checkpoint cÃ³ structure {'model': ..., 'optimizer': ...}
            if isinstance(checkpoint, dict) and 'model' in checkpoint:
                print(f"âœ“ Detected checkpoint format (with 'model' key)")
                state_dict = checkpoint['model']
                if 'epoch' in checkpoint:
                    print(f"  Checkpoint from epoch: {checkpoint['epoch']}")
            else:
                state_dict = checkpoint
            
            # Tá»± Ä‘á»™ng detect config tá»« weights
            print(f"ðŸ” Detecting architecture from weights...")
            
            # PhÃ¢n tÃ­ch keys Ä‘á»ƒ suy ra config
            sample_keys = list(state_dict.keys())[:10]
            print(f"Sample keys: {sample_keys[:3]}")
            
            # Detect hidden_size tá»« embedding weights
            embed_key = None
            for k in state_dict.keys():
                if 'embeddings.patch_embeddings.projection.weight' in k or 'patch_embed.proj.weight' in k:
                    embed_key = k
                    break
            
            if embed_key:
                hidden_size = state_dict[embed_key].shape[0]
                print(f"âœ“ Detected hidden_size: {hidden_size}")
            else:
                hidden_size = 768  # Default ViT-Base
                print(f"âš ï¸  Could not detect hidden_size, using default: {hidden_size}")
            
            # Detect num_layers tá»« sá»‘ lÆ°á»£ng attention layers
            num_layers = 0
            for k in state_dict.keys():
                if 'encoder.layer' in k or 'blocks' in k:
                    # Extract layer index
                    if 'encoder.layer.' in k:
                        layer_idx = int(k.split('encoder.layer.')[1].split('.')[0])
                    elif 'blocks.' in k:
                        layer_idx = int(k.split('blocks.')[1].split('.')[0])
                    else:
                        continue
                    num_layers = max(num_layers, layer_idx + 1)
            
            if num_layers == 0:
                num_layers = 12  # Default ViT-Base
                print(f"âš ï¸  Could not detect num_layers, using default: {num_layers}")
            else:
                print(f"âœ“ Detected num_layers: {num_layers}")
            
            # Táº¡o config dá»±a trÃªn detected values
            config = ViTConfig(
                hidden_size=hidden_size,
                num_hidden_layers=num_layers,
                num_attention_heads=hidden_size // 64,  # ThÆ°á»ng lÃ  hidden_size / 64
                intermediate_size=hidden_size * 4,      # ThÆ°á»ng lÃ  4x hidden_size
                hidden_dropout_prob=0.0,
                attention_probs_dropout_prob=0.0,
                image_size=224,
                patch_size=16,
                num_channels=3
            )
            
            print(f"ðŸ”§ Creating ViT with config: hidden={config.hidden_size}, layers={config.num_hidden_layers}, heads={config.num_attention_heads}")
            
            # Khá»Ÿi táº¡o model tá»« config (khÃ´ng download)
            self.vit = ViTModel(config)
            
            # MedMAE weights cÃ³ thá»ƒ cÃ³ prefix 'encoder.' hoáº·c 'vit.' hoáº·c 'model.'
            # Cáº§n xá»­ lÃ½ Ä‘á»ƒ match vá»›i ViTModel
            new_state_dict = {}
            for k, v in state_dict.items():
                # Loáº¡i bá» prefix náº¿u cÃ³
                new_key = k.replace('encoder.', '').replace('vit.', '').replace('model.', '')
                new_state_dict[new_key] = v
            
            # Load weights (strict=False Ä‘á»ƒ bá» qua cÃ¡c keys khÃ´ng khá»›p nhÆ° decoder)
            missing_keys, unexpected_keys = self.vit.load_state_dict(new_state_dict, strict=False)
            print(f"âœ… Loaded MedMAE weights successfully")
            if len(missing_keys) > 0:
                print(f"âš ï¸  Missing keys: {len(missing_keys)} (this is normal if MedMAE has different head)")
            if len(unexpected_keys) > 0:
                print(f"âš ï¸  Unexpected keys: {len(unexpected_keys)} (decoder weights will be ignored)")
        else:
            # Load trá»±c tiáº¿p tá»« HuggingFace
            print(f"ðŸ“¥ Loading model from HuggingFace: {model_name}")
            self.vit = ViTModel.from_pretrained(model_name)
        
        # Láº¥y hidden dimension (vÃ­ dá»¥: ViT-Base lÃ  768)
        self.embed_dim = self.vit.config.hidden_size
        
        # Patch size (thÆ°á»ng lÃ  16)
        self.patch_size = self.vit.config.patch_size
        
    def forward(self, x):
        """
        Args:
            x: Input image (B, 3, H, W). VÃ­ dá»¥: 224x224
        Returns:
            feature_map: (B, Embed_Dim, Grid_H, Grid_W). VÃ­ dá»¥: (B, 768, 14, 14)
        """
        # 1. Forward qua ViT
        # outputs.last_hidden_state shape: (B, Sequence_Length, Hidden_Dim)
        # Sequence_Length = 1 (CLS token) + (H*W)/(P*P) patches
        outputs = self.vit(x)
        last_hidden_state = outputs.last_hidden_state
        
        # 2. Loáº¡i bá» CLS token (token Ä‘áº§u tiÃªn dÃ¹ng Ä‘á»ƒ phÃ¢n loáº¡i chung)
        # ChÃºng ta cáº§n features cá»§a tá»«ng vÃ¹ng áº£nh cho CSR
        patch_tokens = last_hidden_state[:, 1:, :] # (B, 196, 768) vá»›i áº£nh 224, patch 16
        
        # 3. Reshape tá»« Sequence vá» 2D Grid
        # TÃ­nh kÃ­ch thÆ°á»›c lÆ°á»›i grid: H_grid = H_img // patch_size
        B, N, C = patch_tokens.shape
        H_grid = int(N**0.5) # CÄƒn báº­c 2 cá»§a sá»‘ patch (vÃ­ dá»¥: cÄƒn(196) = 14)
        W_grid = H_grid
        
        # Permute Ä‘á»ƒ Ä‘Æ°a Channel lÃªn trÆ°á»›c: (B, H*W, C) -> (B, C, H*W)
        patch_tokens = patch_tokens.permute(0, 2, 1)
        
        # View láº¡i thÃ nh 2D: (B, C, H_grid, W_grid)
        feature_map = patch_tokens.view(B, C, H_grid, W_grid)
        
        return feature_map

    @property
    def out_channels(self):
        # Property Ä‘á»ƒ CSR biáº¿t dimension Ä‘áº§u ra (thay vÃ¬ fc.in_features cá»§a ResNet)
        return self.embed_dim