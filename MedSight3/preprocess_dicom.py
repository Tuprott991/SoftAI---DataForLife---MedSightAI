"""
Script Ä‘á»ƒ preprocess DICOM thÃ nh PNG/JPG nhanh nháº¥t cÃ³ thá»ƒ.
Sá»­ dá»¥ng multiprocessing Ä‘á»ƒ xá»­ lÃ½ song song trÃªn nhiá»u CPU cores.

Usage:
    python preprocess_dicom.py --input_dir train/ --output_dir train_png/ --num_workers 8
"""

import os
import argparse
from pathlib import Path
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import pandas as pd
import pydicom
import numpy as np
import cv2

def process_single_dicom(args):
    """
    Xá»­ lÃ½ 1 file DICOM thÃ nh PNG.
    Args: tuple (dicom_path, output_path, target_size, image_id)
    Returns: tuple (success, error_msg, resize_info)
        resize_info: dict with image_id, original_height, original_width, resize_factor_h, resize_factor_w
    """
    dicom_path, output_path, target_size, image_id = args
    
    try:
        # Kiá»ƒm tra náº¿u output Ä‘Ã£ tá»“n táº¡i â†’ skip (nhÆ°ng váº«n cáº§n láº¥y resize info)
        dicom = pydicom.dcmread(dicom_path)
        pixel_array = dicom.pixel_array
        
        # LÆ°u original size
        original_h, original_w = pixel_array.shape[:2]
        
        if os.path.exists(output_path):
            # File Ä‘Ã£ tá»“n táº¡i, chá»‰ tráº£ vá» resize info
            resize_factor_h = target_size[1] / original_h
            resize_factor_w = target_size[0] / original_w
            resize_info = {
                'image_id': image_id,
                'original_height': original_h,
                'original_width': original_w,
                'target_height': target_size[1],
                'target_width': target_size[0],
                'resize_factor_h': resize_factor_h,
                'resize_factor_w': resize_factor_w
            }
            return True, None, resize_info
        
        # Photometric Interpretation handling (quan trá»ng cho X-Ray)
        if dicom.PhotometricInterpretation == "MONOCHROME1":
            pixel_array = np.max(pixel_array) - pixel_array
            
        # Normalize vá» 0-255
        pixel_array = pixel_array.astype(np.float32)
        pixel_array = (pixel_array - pixel_array.min()) / (pixel_array.max() - pixel_array.min()) * 255.0
        pixel_array = pixel_array.astype(np.uint8)
        
        # Resize
        img = cv2.resize(pixel_array, target_size)
        
        # Save PNG
        cv2.imwrite(output_path, img)
        
        # Calculate resize factors
        resize_factor_h = target_size[1] / original_h
        resize_factor_w = target_size[0] / original_w
        
        resize_info = {
            'image_id': image_id,
            'original_height': original_h,
            'original_width': original_w,
            'target_height': target_size[1],
            'target_width': target_size[0],
            'resize_factor_h': resize_factor_h,
            'resize_factor_w': resize_factor_w
        }
        
        return True, None, resize_info
    except Exception as e:
        return False, f"{dicom_path}: {str(e)}", None

def preprocess_dataset(input_dir, output_dir, csv_file=None, num_workers=None, format='png', size=224):
    """
    Preprocess toÃ n bá»™ dataset DICOM thÃ nh PNG/JPG.
    
    Args:
        input_dir: Folder chá»©a DICOM files
        output_dir: Folder output cho PNG/JPG
        csv_file: File CSV chá»©a danh sÃ¡ch image_id (optional, náº¿u None sáº½ process táº¥t cáº£)
        num_workers: Sá»‘ CPU cores dÃ¹ng (None = auto detect)
        format: 'png' hoáº·c 'jpg'
        size: Target size for resizing (default: 224)
    """
    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Auto detect sá»‘ workers
    if num_workers is None:
        num_workers = max(1, cpu_count() - 2)  # Äá»ƒ láº¡i 2 cores cho system
    
    print(f"ðŸš€ Starting preprocessing with {num_workers} workers...")
    print(f"ðŸ“‚ Input: {input_dir}")
    print(f"ðŸ“‚ Output: {output_dir}")
    print(f"ðŸŽ¨ Format: {format.upper()}")
    print(f"ðŸ“ Target size: {size}x{size}")
    
    # Láº¥y danh sÃ¡ch files cáº§n xá»­ lÃ½
    if csv_file and os.path.exists(csv_file):
        # Náº¿u cÃ³ CSV, chá»‰ process cÃ¡c áº£nh trong CSV
        df = pd.read_csv(csv_file)
        image_ids = df['image_id'].unique()
        print(f"ðŸ“‹ Found {len(image_ids)} unique images in CSV")
    else:
        # Náº¿u khÃ´ng cÃ³ CSV, process táº¥t cáº£ files trong folder
        image_ids = [f.stem for f in Path(input_dir).glob('*.dicom')]
        if len(image_ids) == 0:
            # Try without extension
            image_ids = [f.stem for f in Path(input_dir).iterdir() if f.is_file()]
        print(f"ðŸ“‹ Found {len(image_ids)} DICOM files in directory")
    
    # Chuáº©n bá»‹ tasks cho multiprocessing
    tasks = []
    for img_id in image_ids:
        # TÃ¬m file DICOM (cÃ³ thá»ƒ cÃ³ hoáº·c khÃ´ng cÃ³ extension)
        dicom_path = Path(input_dir) / f"{img_id}.dicom"
        if not dicom_path.exists():
            dicom_path = Path(input_dir) / img_id
        
        if not dicom_path.exists():
            print(f"âš ï¸  Skip {img_id}: file not found")
            continue
        
        output_path = Path(output_dir) / f"{img_id}.{format}"
        tasks.append((str(dicom_path), str(output_path), (size, size), img_id))
    
    print(f"ðŸ“¦ Total tasks: {len(tasks)}")
    
    # Process vá»›i multiprocessing + progress bar
    success_count = 0
    error_count = 0
    errors = []
    resize_data = []
    
    with Pool(processes=num_workers) as pool:
        results = list(tqdm(
            pool.imap(process_single_dicom, tasks),
            total=len(tasks),
            desc="Processing",
            unit="images"
        ))
    
    # Äáº¿m káº¿t quáº£ vÃ  thu tháº­p resize info
    for success, error_msg, resize_info in results:
        if success:
            success_count += 1
            if resize_info:
                resize_data.append(resize_info)
        else:
            error_count += 1
            if error_msg:
                errors.append(error_msg)
    
    # In káº¿t quáº£
    print(f"\nâœ… Successfully processed: {success_count}/{len(tasks)}")
    if error_count > 0:
        print(f"âŒ Errors: {error_count}")
        print("\nFirst 10 errors:")
        for error in errors[:10]:
            print(f"  - {error}")
    
    print(f"\nðŸŽ‰ Done! Output saved to: {output_dir}")
    
    # LÆ°u resize factors vÃ o CSV
    if resize_data:
        resize_csv_path = Path(output_dir) / 'resize_factors.csv'
        resize_df = pd.DataFrame(resize_data)
        resize_df.to_csv(resize_csv_path, index=False)
        print(f"ðŸ“Š Resize factors saved to: {resize_csv_path}")
        print(f"   Columns: {list(resize_df.columns)}")
    
    # TÃ­nh dung lÆ°á»£ng tiáº¿t kiá»‡m
    if success_count > 0:
        sample_dicom = Path(tasks[0][0])
        sample_png = Path(tasks[0][1])
        if sample_dicom.exists() and sample_png.exists():
            dicom_size = sample_dicom.stat().st_size / 1024 / 1024  # MB
            png_size = sample_png.stat().st_size / 1024 / 1024
            ratio = (dicom_size / png_size) if png_size > 0 else 0
            print(f"ðŸ“Š Size comparison (1 sample):")
            print(f"   DICOM: {dicom_size:.2f} MB")
            print(f"   {format.upper()}: {png_size:.2f} MB")
            print(f"   Ratio: {ratio:.1f}x")

def main():
    parser = argparse.ArgumentParser(description='Preprocess DICOM to PNG/JPG for faster training')
    
    parser.add_argument('--input_dir', type=str, required=True, 
                        help='Input directory containing DICOM files')
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Output directory for PNG/JPG files')
    parser.add_argument('--csv_file', type=str, default=None,
                        help='CSV file with image_id column (optional)')
    parser.add_argument('--num_workers', type=int, default=None,
                        help='Number of CPU workers (default: auto)')
    parser.add_argument('--format', type=str, default='png', choices=['png', 'jpg'],
                        help='Output format: png or jpg (default: png)')
    parser.add_argument('--size', type=int, default=224,
                        help='Target size for resizing (default: 224)')
    
    args = parser.parse_args()
    
    preprocess_dataset(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        csv_file=args.csv_file,
        num_workers=args.num_workers,
        format=args.format,
        size=args.size
    )

if __name__ == '__main__':
    main()
