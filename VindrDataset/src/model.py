import torch
import torch.nn as nn
import torch.nn.functional as F
import timm

class CSRModel(nn.Module):
    def __init__(self, num_classes=7, num_prototypes=5, model_name="densenet121", pretrained=True):
        """
        CSR Model vá»›i DenseNet121 backbone
        
        Args:
            num_classes: Sá»‘ class (7 sau khi loáº¡i bá»)
            num_prototypes: Sá»‘ prototypes má»—i class
            model_name: 'densenet121', 'densenet169', 'densenet201'
            pretrained: Sá»­ dá»¥ng ImageNet pretrained weights
        """
        super().__init__()
        
        self.num_classes = num_classes
        self.num_prototypes = num_prototypes
        
        # --- PHáº¦N 1: BACKBONE (DenseNet121) ---
        self.backbone = timm.create_model(
            model_name, 
            pretrained=pretrained, 
            features_only=True, 
            out_indices=(4,)  # Láº¥y output cá»§a block cuá»‘i
        )
        feature_info = self.backbone.feature_info.get_dicts()[-1]
        self.feature_dim = feature_info["num_chs"]
        print(f"ðŸ”§ Backbone: {model_name}, feature_dim={self.feature_dim}")

        # --- PHáº¦N 2: CONCEPT HEAD ---
        # C: Concept Head (Táº¡o CAMs) - output = num_classes
        self.concept_head = nn.Conv2d(self.feature_dim, num_classes, kernel_size=1)

        # --- PHáº¦N 3: PROTOTYPES ---
        self.embedding_dim = 128
        
        # P: Projector (Chiáº¿u feature vá» khÃ´ng gian contrastive)
        self.projector = nn.Sequential(
            nn.Linear(self.feature_dim, 512),
            nn.ReLU(),
            nn.Linear(512, self.embedding_dim)
        )
        
        # Learnable Prototypes: [Num_Classes, Num_Prototypes_Per_Class, Emb_Dim]
        self.prototypes = nn.Parameter(
            torch.randn(num_classes, num_prototypes, self.embedding_dim)
        )
        
        # --- PHáº¦N 4: TASK HEAD ---
        # H: Task Head (Dá»± Ä‘oÃ¡n bá»‡nh tá»« Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng)
        # Input: [Num_Classes * Num_Prototypes]
        self.task_head = nn.Linear(num_classes * num_prototypes, num_classes)

    def get_features_and_cam(self, x):
        """
        TrÃ­ch xuáº¥t features vÃ  CAM (DÃ¹ng cho Phase 1)
        
        Returns:
            features: [B, C, H, W] - Feature map tá»« backbone
            attn_logits: [B, K, H, W] - CAM logits cho má»—i class
        """
        # Chuyá»ƒn grayscale sang RGB náº¿u cáº§n
        if x.size(1) == 1:
            x = x.repeat(1, 3, 1, 1)
            
        features = self.backbone(x)[0]          # [B, feature_dim, H, W]
        attn_logits = self.concept_head(features)  # [B, num_classes, H, W]
        return features, attn_logits

    def get_projected_vectors(self, features, attn_logits):
        """
        Láº¥y Local Concept Vectors v^k (DÃ¹ng cho Phase 2)
        
        Args:
            features: [B, C, H, W]
            attn_logits: [B, K, H, W]
            
        Returns:
            projected_vectors: [B, K, Emb_Dim] - Normalized projected vectors
        """
        B, C, H, W = features.shape
        K = attn_logits.shape[1]  # num_classes
        
        # 1. Normalize CAM (Spatial Softmax)
        attn_weights = F.softmax(attn_logits.view(B, K, -1), dim=-1).view(B, K, H, W)
        
        # 2. Weighted Sum Ä‘á»ƒ láº¥y vector Ä‘áº¡i diá»‡n cho tá»«ng concept
        # features: [B, C, H*W] -> [B, H*W, C]
        features_flat = features.view(B, C, -1).permute(0, 2, 1)
        
        # v = weights * features -> [B, K, C]
        local_concept_vectors = torch.bmm(attn_weights.view(B, K, -1), features_flat)
        
        # 3. Project sang khÃ´ng gian embedding -> v'
        projected_vectors = self.projector(local_concept_vectors)  # [B, K, Emb_Dim]
        
        return F.normalize(projected_vectors, p=2, dim=-1)

    def forward(self, x):
        """
        Full forward pass (DÃ¹ng cho Phase 3 & Inference)
        
        Returns:
            dict vá»›i:
            - logits: [B, num_classes] - Final predictions
            - attn_maps: [B, K, H, W] - CAM maps
            - projected_vectors: [B, K, Emb_Dim]
            - sim_scores: [B, K, M] - Similarity vá»›i prototypes
        """
        # 1. TrÃ­ch xuáº¥t features & CAM
        features, attn_logits = self.get_features_and_cam(x)
        
        # 2. Get projected vectors
        projected_vectors = self.get_projected_vectors(features, attn_logits)  # [B, K, Emb_Dim]
        
        # 3. TÃ­nh Similarity Score vá»›i Prototypes
        # Prototypes: [K, M, Emb] -> normalize
        prototypes_norm = F.normalize(self.prototypes, p=2, dim=-1)
        
        # Cosine Similarity: [B, K, M]
        sim_scores = torch.einsum('bkc,kmc->bkm', projected_vectors, prototypes_norm)
        
        # 4. Flatten thÃ nh vector s [B, K*M]
        s_vector = sim_scores.reshape(x.size(0), -1)
        
        # 5. Predict tá»« similarity scores
        logits = self.task_head(s_vector)
        
        return {
            "logits": logits,
            "attn_maps": attn_logits,
            "projected_vectors": projected_vectors,
            "sim_scores": sim_scores
        }


# # Test model
# if __name__ == "__main__":
#     # Test vá»›i input giáº£
#     model = CSRModel(num_classes=7, num_prototypes=15, model_name="densenet121")
    
#     # Input: [Batch, Channel, H, W]
#     x = torch.randn(2, 1, 384, 384)  # Grayscale input
    
#     output = model(x)
    
#     print(f"Input shape: {x.shape}")
#     print(f"Logits shape: {output['logits'].shape}")           # [2, 7]
#     print(f"Attn maps shape: {output['attn_maps'].shape}")     # [2, 7, 12, 12]
#     print(f"Projected vectors: {output['projected_vectors'].shape}")  # [2, 7, 128]
#     print(f"Sim scores: {output['sim_scores'].shape}")         # [2, 7, 15]